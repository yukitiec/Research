{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPH7Eh541XjLRAij9cioEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukitiec/Research/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Library"
      ],
      "metadata": {
        "id": "Qi8xeP1IduVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BTIeYcOVdpP-"
      },
      "outputs": [],
      "source": [
        "import torch\\\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "lzIDDcSMHg6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Module"
      ],
      "metadata": {
        "id": "63q21x0mdz16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input Layer"
      ],
      "metadata": {
        "id": "jO4nAU9Ud9aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VitInputLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels:int = 1,\n",
        "               emb_dim:int = 384,\n",
        "               num_patch_row:int = 2,\n",
        "               image_size:int = 28):\n",
        "    \"\"\"\n",
        "    in_channels : num of channels of input images\n",
        "    emb_dim : length of vector after embedded\n",
        "    num_patch_row : num of patch in height axis\n",
        "    image size : image size\n",
        "    \"\"\"\n",
        "    super(VitInputLayer,self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.emb_dim = emb_dim\n",
        "    self.num_patch_row = num_patch_row\n",
        "    self.image_size = image_size\n",
        "\n",
        "    #num of patch \n",
        "    self.num_patch = self.num_patch_row**2\n",
        "\n",
        "    #size of patch\n",
        "    self.patch_size = int(self.image_size//self.num_patch_row)\n",
        "\n",
        "    #make input images into patch and embedded one with Conv2D\n",
        "    self.patch_emb_layer = nn.Conv2d(\n",
        "        in_channels = self.in_channels,\n",
        "        out_channels = self.emb_dim,\n",
        "        kernel_size = self.patch_size,\n",
        "        stride = self.patch_size\n",
        "    )\n",
        "\n",
        "    #class token\n",
        "    self.cls_token = nn.Parameter(\n",
        "        torch.randn(1,1,emb_dim)\n",
        "    )\n",
        "\n",
        "    #positional embedding\n",
        "    #prepare (batch_size+1) vectors for embedded vectors because the header is class token\n",
        "    self.pos_emb = nn.Parameter(\n",
        "        torch.randn(1,self.num_patch+1,emb_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x : input image (B,C,H,W)\n",
        "        B:Batch size, C:Channel, H:Height, W:Width\n",
        "\n",
        "\n",
        "    Return:\n",
        "      z_0 : input for Vit (B,N,D)\n",
        "        B:Batch size, N : Num of Token D: Length of embedded vectors\n",
        "    \"\"\"\n",
        "\n",
        "    #Patch embedding\n",
        "    #(B,C,H,W) -> (B,C,H/P,W/P)\n",
        "    z_0 = self.patch_emb_layer(x)\n",
        "\n",
        "    #patch flatten\n",
        "    #(B,C,H/P,W/P) -> (B,D，Np) (D = (P^2*C), after 2 is flatten)\n",
        "    z_0 = z_0.flatten(2)\n",
        "\n",
        "    #reshape the matrix\n",
        "    #(B,D,Np) -> (B,Np,D)\n",
        "    z_0 = z_0.transpose(1,2)\n",
        "\n",
        "    #Concatenate class token at the head of embeddings\n",
        "    #(B,Np,D) -> (B,N,D) N = Np + 1\n",
        "    #cls token : (1,1,D) -> (B,1,D)\n",
        "    z_0 = torch.cat(\n",
        "        [self.cls_token.repeat(repeats = (x.size(0),1,1)), z_0], dim=1\n",
        "    )\n",
        "\n",
        "    #Positional Embedding\n",
        "    #(B,N,D) -> (B,N,D)\n",
        "    z_0 = z_0 + self.pos_emb\n",
        "\n",
        "    return z_0"
      ],
      "metadata": {
        "id": "aewTo3BpdzMH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "9DE4KGnEI5nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_dim:int = 384,\n",
        "               head:int = 3,\n",
        "               dropout:float = 0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      emb_dim : the length of embedded vector\n",
        "      head : num of head\n",
        "      dropout : the rate of dropout\n",
        "    \"\"\"\n",
        "\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.head = head\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_dim = emb_dim // head\n",
        "    self.sqrt_dh = self.head_dim**0.5 #for attention weight\n",
        "\n",
        "    #linear layer for q,k,v\n",
        "    self.w_q = nn.Linear(emb_dim,emb_dim,bias=False)\n",
        "    self.w_k = nn.Linear(emb_dim,emb_dim,bias=False)\n",
        "    self.w_v = nn.Linear(emb_dim,emb_dim,bias=False)\n",
        "\n",
        "    #dropout layer\n",
        "    self.attn_drop = nn.Dropout(dropout)\n",
        "\n",
        "    #linear layer for ouput of MHSA\n",
        "    self.w_o = nn.Sequential(\n",
        "        nn.Linear(emb_dim,emb_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, z:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      z: input for MHSA (B,N,D)\n",
        "        B:Batch size, N: Num of patches, D:length of embedded vectors\n",
        "    \n",
        "    Return:\n",
        "      out: output of MHSA (B,N,D)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, num_patch, _ = z.size()\n",
        "\n",
        "    #embedding\n",
        "    q = self.w_q(z)\n",
        "    k = self.w_k(z)\n",
        "    v = self.w_v(z)\n",
        "\n",
        "    #split q,k,v for MHSA\n",
        "    #(B,N,D) -> (B,N,h,D//h)\n",
        "    q = q.view(batch_size,num_patch,self.head,self.head_dim)\n",
        "    k = k.view(batch_size,num_patch,self.head,self.head_dim)\n",
        "    v = v.view(batch_size,num_patch,self.head,self.head_dim)\n",
        "\n",
        "    #arrange data for self-attention\n",
        "    #(B,N,h,D//h) -> (B,h,N,D//h)\n",
        "    q = q.transpose(1,2)\n",
        "    k = k.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    #arragen k for attention weight \n",
        "    #(B,h,N,D//h) -> (B,h,h//D,N) \n",
        "    k_T = k.transpose(2,3)\n",
        "    #inner dot\n",
        "    #(B,h,N,D//h)*(B,h,h//D,N) -> (B,h,N,N)\n",
        "    dots = (q@k_T)/self.sqrt_dh\n",
        "    #softmax in row axis\n",
        "    attn = F.softmax(dots,dim=-1)\n",
        "    #Dropout\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    #get new embeddings\n",
        "    #(B,h,N,N)*(B,h,N,D//h) -> (B,h,N,D//h)\n",
        "    out = attn@v\n",
        "    #(B,h,N,D//h) -> (B,N,h,D//h)\n",
        "    out = out.transpose(1,2)\n",
        "    #(B,N,h,D//h) -> (B,N,D)\n",
        "    out = out.reshape(batch_size,num_patch,self.emb_dim)\n",
        "\n",
        "    #output layer\n",
        "    out = self.w_o(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ePpxZh67I5QK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "dYa-pdJfU3J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VitEncoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "                emb_dim:int = 384,\n",
        "                head:int = 8,\n",
        "                hidden_dim:int = 384*4,\n",
        "                dropout:float = 0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      emb_dim : length of embedded vectors\n",
        "      head : num of heads in MHSA\n",
        "      hidden_dim : length of the middle layer of MLP in Encoder Block, here 384*4 as in paper\n",
        "      dropout : dropout rate\n",
        "    \"\"\"\n",
        "\n",
        "    super(VitEncoderBlock,self).__init__()\n",
        "    #first LayerNormalization\n",
        "    self.ln1 = nn.LayerNorm(emb_dim)\n",
        "    #MHSA\n",
        "    self.msa = MultiHeadSelfAttention(\n",
        "        emb_dim = emb_dim,\n",
        "        head = head,\n",
        "        dropout = dropout\n",
        "    )\n",
        "\n",
        "    #second LayerNormalization\n",
        "    self.ln2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    #MLP\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(emb_dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim,emb_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,z:torch.Tensor) ->  torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      z : input for Encoder Block (B,N,D)\n",
        "    \n",
        "    Return:\n",
        "      out:out for Encoder Block (B,N,D)\n",
        "    \"\"\"\n",
        "    #first half\n",
        "    out = self.msa(self.ln1(z)) + z\n",
        "    #second half\n",
        "    out = self.mlp(self.ln2(out)) + out\n",
        "    return out"
      ],
      "metadata": {
        "id": "8G9TgL6IU20M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking Code"
      ],
      "metadata": {
        "id": "WCVwu7zqrhO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, channel, height, width = 2,3,32,32\n",
        "x = torch.randn(batch_size, channel, height, width)\n",
        "input_layer = VitInputLayer(num_patch_row = 2)\n",
        "z_0 = input_layer(x)\n",
        "\n",
        "#check if the shape is (2,5,384)\n",
        "print(\"after input layer\")\n",
        "print(z_0.shape)\n",
        "\n",
        "#MHSA\n",
        "mhsa = MultiHeadSelfAttention()\n",
        "out = mhsa(z_0)\n",
        "\n",
        "print(\"after MHSA layer\")\n",
        "print(out.shape)\n",
        "\n",
        "vit_enc = VitEncoderBlock()\n",
        "z_1 = vit_enc(z_0)\n",
        "\n",
        "print(\"after Vit Encoder Block\")\n",
        "print(z_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjUfeWAqrgeH",
        "outputId": "47f48205-6a90-48cb-845a-167e0794a3d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after input layer\n",
            "torch.Size([2, 5, 384])\n",
            "after MHSA layer\n",
            "torch.Size([2, 5, 384])\n",
            "after Vit Encoder Block\n",
            "torch.Size([2, 5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visual Transformer"
      ],
      "metadata": {
        "id": "a9gu2JqClJa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vit(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels:int = 1,\n",
        "               num_classes:int = 10,\n",
        "               emb_dim:int = 384,\n",
        "               num_patch_row:int = 2,\n",
        "               image_size:int = 28,\n",
        "               num_blocks:int = 7,\n",
        "               head:int = 8,\n",
        "               hidden_dim:int = 384*4,\n",
        "               dropout:float = 0.):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      in_channesl : num of channels of input image\n",
        "      num_classes : num of classes\n",
        "      emb_dim : length of embedded vectors\n",
        "      num_patch_row : num of patch per row\n",
        "      image_size : image length\n",
        "      num_blocks : num of Encoder Block\n",
        "      head : num of heads\n",
        "      hidden_dim : length of middle layer in Encoder block\n",
        "      dropout : rate of dropout\n",
        "    \"\"\"\n",
        "    super(Vit, self).__init__()\n",
        "\n",
        "    #Input layer\n",
        "    self.input_layer = VitInputLayer(\n",
        "        in_channels,\n",
        "        emb_dim,\n",
        "        num_patch_row,\n",
        "        image_size\n",
        "    )\n",
        "\n",
        "    #Encoder\n",
        "    self.encoder = nn.Sequential(*[\n",
        "        VitEncoderBlock(\n",
        "            emb_dim = emb_dim,\n",
        "            head = head,\n",
        "            hidden_dim = hidden_dim,\n",
        "            dropout = dropout\n",
        "        )\n",
        "        for _ in range(num_blocks)\n",
        "    ])\n",
        "\n",
        "    #MLP Head\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(emb_dim),\n",
        "        nn.Linear(emb_dim,num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x : input for Visual Transformer (B,C,H,W)\n",
        "        B:Batch size, C:Num of Channels, H:Height, W:Width\n",
        "    \n",
        "    Return:\n",
        "      out : output of ViT (B,M)\n",
        "        B:Batch size, M: num of classes\n",
        "    \"\"\"\n",
        "\n",
        "    #Input layer\n",
        "    #(B,C,H,W) -> (B,N,D)\n",
        "    #N : (Num of tokens) + 1(Class), D : length of embedded vecotor\n",
        "    out = self.input_layer(x)\n",
        "\n",
        "    #Encoder\n",
        "    #(B,N,D) -> (B,N,D)\n",
        "    out = self.encoder(out)\n",
        "    \n",
        "    #extract class token\n",
        "    #(B,N,D) -> (B,D)\n",
        "    cls_token = out[:,0]\n",
        "    \n",
        "    #MLP\n",
        "    #(B,D) -> (B,M)\n",
        "    pred = self.mlp_head(cls_token)\n",
        "    \n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "kpcLRY1olMcR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking Code"
      ],
      "metadata": {
        "id": "mesUT720rlON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "batch_size, channel, height, width = 2,3,32,32\n",
        "x = torch.randn(batch_size,channel,height,width)\n",
        "print(x.shape)\n",
        "vit = Vit(in_channels = channel,num_classes = num_classes)\n",
        "pred = vit(x)\n",
        "\n",
        "print(pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zakeW55-rnJB",
        "outputId": "8e5b230b-4b5a-4bec-b64c-2e0c22f98ab7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 32, 32])\n",
            "torch.Size([2, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Load"
      ],
      "metadata": {
        "id": "jdEYH0xX6cU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "mdEJe7gU6e3c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkNF0dXw67O0",
        "outputId": "d366ac5f-494c-4a83-9a99-d61903ac76ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8569233.42it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 142621.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2639738.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 19244453.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx5oqsMJ7WDw",
        "outputId": "fb3eaf4c-20ba-4b44-aa65-4255daa4abeb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup device"
      ],
      "metadata": {
        "id": "ziXeP2Ef8EsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2ZEBsXs8HBL",
        "outputId": "eff9b169-76cd-4e4a-a77f-4a0e3633cbfb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Session"
      ],
      "metadata": {
        "id": "3geIUbxBjFqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model setting"
      ],
      "metadata": {
        "id": "Q8l0qpab9S2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Vit().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaOm7yFjjEj-",
        "outputId": "e81eb32f-2952-4fac-a4cb-621548340fa2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vit(\n",
            "  (input_layer): VitInputLayer(\n",
            "    (patch_emb_layer): Conv2d(1, 384, kernel_size=(14, 14), stride=(14, 14))\n",
            "  )\n",
            "  (encoder): Sequential(\n",
            "    (0): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (6): VitEncoderBlock(\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (msa): MultiHeadSelfAttention(\n",
            "        (w_q): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_k): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (w_v): Linear(in_features=384, out_features=384, bias=False)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (w_o): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (1): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mlp_head): Sequential(\n",
            "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=384, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training setup"
      ],
      "metadata": {
        "id": "30JGZ0E89WY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "#torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0,"
      ],
      "metadata": {
        "id": "bRV1EMADjtQ6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilGyzb0i92qu",
        "outputId": "a35f37d6-43dc-4543-f1b6-68a9626bb372"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7fa4ec1ad7e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train session & Test session"
      ],
      "metadata": {
        "id": "pCIN_ort-LyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "vlSsE9Ne91B6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr5ouusx-Pk1",
        "outputId": "002d3523-97a7-461b-faf1-a93bc54b49c4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.331674  [   64/60000]\n",
            "loss: 0.914306  [ 6464/60000]\n",
            "loss: 0.511618  [12864/60000]\n",
            "loss: 0.944841  [19264/60000]\n",
            "loss: 0.728349  [25664/60000]\n",
            "loss: 0.587158  [32064/60000]\n",
            "loss: 0.670818  [38464/60000]\n",
            "loss: 0.586845  [44864/60000]\n",
            "loss: 0.696419  [51264/60000]\n",
            "loss: 0.628529  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.570324 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.443964  [   64/60000]\n",
            "loss: 0.513597  [ 6464/60000]\n",
            "loss: 0.464759  [12864/60000]\n",
            "loss: 0.491383  [19264/60000]\n",
            "loss: 0.592807  [25664/60000]\n",
            "loss: 0.557748  [32064/60000]\n",
            "loss: 0.442315  [38464/60000]\n",
            "loss: 0.555614  [44864/60000]\n",
            "loss: 0.589858  [51264/60000]\n",
            "loss: 0.551738  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.524524 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.443368  [   64/60000]\n",
            "loss: 0.410286  [ 6464/60000]\n",
            "loss: 0.350519  [12864/60000]\n",
            "loss: 0.546039  [19264/60000]\n",
            "loss: 0.525412  [25664/60000]\n",
            "loss: 0.512137  [32064/60000]\n",
            "loss: 0.513211  [38464/60000]\n",
            "loss: 0.507356  [44864/60000]\n",
            "loss: 0.545209  [51264/60000]\n",
            "loss: 0.503583  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.482277 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.340815  [   64/60000]\n",
            "loss: 0.392070  [ 6464/60000]\n",
            "loss: 0.334787  [12864/60000]\n",
            "loss: 0.497350  [19264/60000]\n",
            "loss: 0.475748  [25664/60000]\n",
            "loss: 0.405913  [32064/60000]\n",
            "loss: 0.435634  [38464/60000]\n",
            "loss: 0.455127  [44864/60000]\n",
            "loss: 0.578945  [51264/60000]\n",
            "loss: 0.454129  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.446693 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.381098  [   64/60000]\n",
            "loss: 0.371497  [ 6464/60000]\n",
            "loss: 0.372599  [12864/60000]\n",
            "loss: 0.456616  [19264/60000]\n",
            "loss: 0.604065  [25664/60000]\n",
            "loss: 0.469002  [32064/60000]\n",
            "loss: 0.411594  [38464/60000]\n",
            "loss: 0.557621  [44864/60000]\n",
            "loss: 0.508097  [51264/60000]\n",
            "loss: 0.439858  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.440710 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving model"
      ],
      "metadata": {
        "id": "mclZuxMQINGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_path = '/content/gdrive/My Drive/YAMAKAWA_LAB/Vit'\n",
        "if not os.path.exists(model_path):\n",
        "  os.makedirs(model_path)\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(model_path,\"model.pth\"))\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "id": "mtMiibnXINGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize hidden layer"
      ],
      "metadata": {
        "id": "AMjXlapFDsuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize positional encoding"
      ],
      "metadata": {
        "id": "6fZ98F5FHBS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#model setting\n",
        "model = vit()\n",
        "#load pretrained model\n",
        "checkpoint = torch.load('/content/gdrive/My Drive/YAMAKAWA_LAB/Vit/model.pth')\n",
        "checkpoint_model = checkpoint[\"model\"]\n",
        "model.load_state_dict(checkpoint_model)\n",
        "\n",
        "#load positional embeddings from model\n",
        "#N : num of patches + 1(cls token), D:dimension\n",
        "pos_embed = model.state_dict()[\"pos_embed\"]#shape:(1,N,D)\n",
        "H_and_W = int(math.sqrt(pos_embed.shape[-1]-1)) #subtract cls token, -> calculate num_patch_row\n",
        "\n",
        "#visualize cosine similarity\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "for i in range(1,pos_embed.shape[1]):\n",
        "  sim = F.cosine_similarity(pos_embed[0,i:i+1],pos_embed[0,1:],dim=1)\n",
        "  sim = sim.reshape((H_and_W,H_and_W)).detach().cpu().numpy() #numpy uses CPU, so change device to CPU when changing tensor to numpy\n",
        "  ax = fig.add_subplot(H_and_W,H_and_W,i)\n",
        "  ax.imshow(sim)\n",
        "plt.savefig(\"/content/gdrive/My Drive/YAMAKAWA_LAB/Vit/position_embedding.pdf\")"
      ],
      "metadata": {
        "id": "u30-iZ_-DwCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize with Attention Rollout"
      ],
      "metadata": {
        "id": "xqtPHB-gQkho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def extract(pre_model,target,inputs):\n",
        "  \"\"\"Extract Attention Weight\n",
        "  \"\"\"\n",
        "  feature = None\n",
        "  def forward_hook(module,target,outputs):\n",
        "    \"\"\"save output of forward ias a global variables\n",
        "    \"\"\"\n",
        "    global blocks\n",
        "    blocks = outputs.detach()\n",
        "\n",
        "  #enroll callback function\n",
        "  handle = target.register_forward_hook(forward_hook)\n",
        "  #inference\n",
        "  pre_model.eval()\n",
        "  pre_model(inputs)\n",
        "  #remove callback\n",
        "  handle.remove()\n",
        "  return blocks\n",
        "\n",
        "#model setting\n",
        "model = vit()\n",
        "#load pretrained model\n",
        "checkpoint = torch.load('/content/gdrive/My Drive/YAMAKAWA_LAB/Vit/model.pth')\n",
        "checkpoint_model = checkpoint[\"model\"]\n",
        "model.load_state_dict(checkpoint_model)\n",
        "\n",
        "#get each Attention weight \n",
        "#L :num of layers, H:num of Heads, N:num of patches + cls token(1)\n",
        "attention_weight = []\n",
        "\n",
        "#resize the images and crop image ceter\n",
        "#change the image shape to fit the model input\n",
        "normalize = transforms.Normalize(mean=[0.5,0.5,0.5],std = [0.5,0.5,0.5])\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(192,192), #64*3\n",
        "    trainsforms.CenterCrop(192,192),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "#load image file\n",
        "image = Image.open(\"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/\") #image name\n",
        "x = transform(image) #shape:(1,3,224,224)\n",
        "\n",
        "for i in range(len(model.blocks)):\n",
        "  target_module = model.blocks[i].attn.attn_drop\n",
        "  features = extract(model, target_module, x) #shape:(1,H,N,N)\n",
        "  attention_weight.append([features.to(\"cpu\").detach.numpy().copy()])\n",
        "attention_weight = np.squeeze(np.concatenate(attention_weight),axis = 1) #shape: (L,H,N,N)\n",
        "\n",
        "#Calculation\n",
        "#Average in head axis\n",
        "mean_head = np.mean(attention_weight,axis=1) #(L,N,N)\n",
        "\n",
        "#add N*N eye matrix to mean_head\n",
        "mean_head = mean_head + np.eye(mean_head.shape[1])\n",
        "\n",
        "#Normalize\n",
        "mean_head = mean_head / mean_head.sum(axis=(1,2))[:,np.newaxis,np.newaxis] #(L,N,N)\n",
        "\n",
        "#multiply in layer axis\n",
        "v = mean_head[-1]\n",
        "for n in range(1,len(mean_head)):\n",
        "  v = np.matmul(v,mean_head[-1-n])\n",
        "\n",
        "#make attention map\n",
        "mask = v[0,1;].reshape(3,3) # num of patches\n",
        "attention_map = cv2.resize(mask/mask.max(), (ori_img.shape[2],ori_img.shape[3]))[...,np.newaxis]\n",
        "\n",
        "#show attention map\n",
        "plt.imshow(attention_map)\n",
        "plt.savefig('/content/gdrive/My Drive/YAMAKAWA_LAB/Vit/attention_map.pdf')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "2d9PFybBQqVy",
        "outputId": "832bd16d-c6a5-4469-af93-ca925fa4166c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-16414827641f>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load model"
      ],
      "metadata": {
        "id": "dh9SK2LR_AC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Vit().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "id": "-BQFZBW3-_nT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}