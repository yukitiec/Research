{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1eo2mW6hgJwhnq4rEVi1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukitiec/Research/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Library"
      ],
      "metadata": {
        "id": "Qi8xeP1IduVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BTIeYcOVdpP-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Module"
      ],
      "metadata": {
        "id": "63q21x0mdz16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input Layer"
      ],
      "metadata": {
        "id": "jO4nAU9Ud9aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VitInputLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels:int = 3,\n",
        "               emb_dim:int = 384,\n",
        "               num_patch_row:int = 2,\n",
        "               image_size:int = 32):\n",
        "    \"\"\"\n",
        "    in_channels : num of channels of input images\n",
        "    emb_dim : length of vector after embedded\n",
        "    num_patch_row : num of patch in height axis\n",
        "    image size : image size\n",
        "    \"\"\"\n",
        "    super(VitInputLayer,self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.emb_dim = emb_dim\n",
        "    self.num_patch_row = num_patch_row\n",
        "    self.image_size = image_size\n",
        "\n",
        "    #num of patch \n",
        "    self.num_patch = self.num_patch_row**2\n",
        "\n",
        "    #size of patch\n",
        "    self.patch_size = int(self.image_size//self.num_patch_row)\n",
        "\n",
        "    #make input images into patch and embedded one with Conv2D\n",
        "    self.patch_emb_layer = nn.Conv2d(\n",
        "        in_channels = self.in_channels,\n",
        "        out_channels = self.emb_dim,\n",
        "        kernel_size = self.patch_size,\n",
        "        stride = self.patch_size\n",
        "    )\n",
        "\n",
        "    #class token\n",
        "    self.cls_token = nn.Parameter(\n",
        "        torch.randn(1,1,emb_dim)\n",
        "    )\n",
        "\n",
        "    #positional embedding\n",
        "    #prepare (batch_size+1) vectors for embedded vectors because the header is class token\n",
        "    self.pos_emb = nn.Parameter(\n",
        "        torch.randn(1,self.num_patch+1,emb_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self,x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x : input image (B,C,H,W)\n",
        "        B:Batch size, C:Channel, H:Height, W:Width\n",
        "\n",
        "\n",
        "    Return:\n",
        "      z_0 : input for Vit (B,N,D)\n",
        "        B:Batch size, N : Num of Token D: Length of embedded vectors\n",
        "    \"\"\"\n",
        "\n",
        "    #Patch embedding\n",
        "    #(B,C,H,W) -> (B,C,H/P,W/P)\n",
        "    z_0 = self.patch_emb_layer(x)\n",
        "\n",
        "    #patch flatten\n",
        "    #(B,C,H/P,W/P) -> (B,Dï¼ŒNp) (D = (P^2*C), after 2 is flatten)\n",
        "    z_0 = z_0.flatten(2)\n",
        "\n",
        "    #reshape the matrix\n",
        "    #(B,D,Np) -> (B,Np,D)\n",
        "    z_0 = z_0.transpose(1,2)\n",
        "\n",
        "    #Concatenate class token at the head of embeddings\n",
        "    #(B,Np,D) -> (B,N,D) N = Np + 1\n",
        "    #cls token : (1,1,D) -> (B,1,D)\n",
        "    z_0 = torch.cat(\n",
        "        [self.cls_token.repeat(repeats = (x.size(0),1,1)), z_0], dim=1\n",
        "    )\n",
        "\n",
        "    #Positional Embedding\n",
        "    #(B,N,D) -> (B,N,D)\n",
        "    z_0 = z_0 + self.pos_emb\n",
        "\n",
        "    return z_0"
      ],
      "metadata": {
        "id": "aewTo3BpdzMH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "9DE4KGnEI5nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               emb_dim:int = 384,\n",
        "               head:int = 3,\n",
        "               dropout:float = 0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      emb_dim : the length of embedded vector\n",
        "      head : num of head\n",
        "      dropout : the rate of dropout\n",
        "    \"\"\"\n",
        "\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.head = head\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_dim = emb_dim // head\n",
        "    self.sqrt_dh = self.head_dim**0.5 #for attention weight\n",
        "\n",
        "    #linear layer for q,k,v\n",
        "    self.w_q = nn.Linear(emb_dim,emb_dim,bias=False)\n",
        "    self.w_k = nn.Linear(emb_dim,emb_dim,bias=False)\n",
        "    self.w_v = nn.Linear(emb_dim,emb_dim,bias=False)\n",
        "\n",
        "    #dropout layer\n",
        "    self.attn_drop = nn.Dropout(dropout)\n",
        "\n",
        "    #linear layer for ouput of MHSA\n",
        "    self.w_o = nn.Sequential(\n",
        "        nn.Linear(emb_dim,emb_dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, z:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      z: input for MHSA (B,N,D)\n",
        "        B:Batch size, N: Num of patches, D:length of embedded vectors\n",
        "    \n",
        "    Return:\n",
        "      out: output of MHSA (B,N,D)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, num_patch, _ = z.size()\n",
        "\n",
        "    #embedding\n",
        "    q = self.w_q(z)\n",
        "    k = self.w_k(z)\n",
        "    v = self.w_v(z)\n",
        "\n",
        "    #split q,k,v for MHSA\n",
        "    #(B,N,D) -> (B,N,h,D//h)\n",
        "    q = q.view(batch_size,num_patch,self.head,self.head_dim)\n",
        "    k = k.view(batch_size,num_patch,self.head,self.head_dim)\n",
        "    v = v.view(batch_size,num_patch,self.head,self.head_dim)\n",
        "\n",
        "    #arrange data for self-attention\n",
        "    #(B,N,h,D//h) -> (B,h,N,D//h)\n",
        "    q = q.transpose(1,2)\n",
        "    k = k.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    #arragen k for attention weight \n",
        "    #(B,h,N,D//h) -> (B,h,h//D,N) \n",
        "    k_T = k.transpose(2,3)\n",
        "    #inner dot\n",
        "    #(B,h,N,D//h)*(B,h,h//D,N) -> (B,h,N,N)\n",
        "    dots = (q@k_T)/self.sqrt_dh\n",
        "    #softmax in row axis\n",
        "    attn = F.softmax(dots,dim=-1)\n",
        "    #Dropout\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    #get new embeddings\n",
        "    #(B,h,N,N)*(B,h,N,D//h) -> (B,h,N,D//h)\n",
        "    out = attn@v\n",
        "    #(B,h,N,D//h) -> (B,N,h,D//h)\n",
        "    out = out.transpose(1,2)\n",
        "    #(B,N,h,D//h) -> (B,N,D)\n",
        "    out = out.reshape(batch_size,num_patch,self.emb_dim)\n",
        "\n",
        "    #output layer\n",
        "    out = self.w_o(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ePpxZh67I5QK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "dYa-pdJfU3J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  class VitEncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_dim:int = 384,\n",
        "                 head:int = 8,\n",
        "                 hidden_dim:int = 384*4,\n",
        "                 dropout:float = 0):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        emb_dim : length of embedded vectors\n",
        "        head : num of heads in MHSA\n",
        "        hidden_dim : length of the middle layer of MLP in Encoder Block, here 384*4 as in paper\n",
        "        dropout : dropout rate\n",
        "      \"\"\"\n",
        "\n",
        "      super(VitEncoderBlock,self).__init__()\n",
        "      #first LayerNormalization\n",
        "      self.ln1 = nn.LayerNorm(emb_dim)\n",
        "      #MHSA\n",
        "      self.msa = MultiHeadSelfAttention(\n",
        "          emb_dim = emb_dim,\n",
        "          head = head,\n",
        "          dropout = dropout\n",
        "      )\n",
        "\n",
        "      #second LayerNormalization\n",
        "      self.ln2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "      #MLP\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(emb_dim, hidden_dim),\n",
        "          nn.GELU(),\n",
        "          nn.Dropout(dropout),\n",
        "          nn.Linear(hidden_dim,emb_dim),\n",
        "          nn.Dropout(dropout)\n",
        "      )\n",
        "\n",
        "    def forward(self,z:torch.Tensor) ->  torch.Tensor:\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        z : input for Encoder Block (B,N,D)\n",
        "      \n",
        "      Return:\n",
        "        out:out for Encoder Block (B,N,D)\n",
        "      \"\"\"\n",
        "      #first half\n",
        "      out = self.msa(self.ln1(z)) + z\n",
        "      #second half\n",
        "      out = self.mlp(self.ln2(out)) + out\n",
        "      return out"
      ],
      "metadata": {
        "id": "8G9TgL6IU20M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visual Transformer"
      ],
      "metadata": {
        "id": "a9gu2JqClJa9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpcLRY1olMcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Session"
      ],
      "metadata": {
        "id": "3geIUbxBjFqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, channel, height, width = 2,3,32,32\n",
        "x = torch.randn(batch_size, channel, height, width)\n",
        "input_layer = VitInputLayer(num_patch_row = 2)\n",
        "z_0 = input_layer(x)\n",
        "\n",
        "#check if the shape is (2,5,384)\n",
        "print(\"after input layer\")\n",
        "print(z_0.shape)\n",
        "\n",
        "#MHSA\n",
        "mhsa = MultiHeadSelfAttention()\n",
        "out = mhsa(z_0)\n",
        "\n",
        "print(\"after MHSA layer\")\n",
        "print(out.shape)\n",
        "\n",
        "vit_enc = VitEncoderBlock()\n",
        "z_1 = vit_enc(z_0)\n",
        "\n",
        "print(\"after Vit Encoder Block\")\n",
        "print(z_1.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaOm7yFjjEj-",
        "outputId": "41a98fe8-9a2b-4872-e8ef-2f100d090c90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after input layer\n",
            "torch.Size([2, 5, 384])\n",
            "after MHSA layer\n",
            "torch.Size([2, 5, 384])\n",
            "after Vit Encoder Block\n",
            "torch.Size([2, 5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRV1EMADjtQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}