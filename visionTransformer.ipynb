{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6bIlJM4HPxevqyrD8ah5K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukitiec/Research/blob/main/visionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPU setting"
      ],
      "metadata": {
        "id": "WMKjXnyED5t-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCfDs2E1DDgL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Library"
      ],
      "metadata": {
        "id": "dbhC4KGgENsq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HuHGOJwOpw2z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras import Model\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense,SimpleRNN, LSTM,Dropout,Convolution2D,MaxPooling2D\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import glob \n",
        "import os \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random #random.shuffle() : shuffle the data of 1D\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_73__HSxqmWi",
        "outputId": "d2bad70f-9943-4d4c-8081-3c488190f246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbZGBZdlwYNw"
      },
      "source": [
        "#Load Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hhBrahf9qoce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f557e14e-a997-4b9e-8369-87229f0659f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0\n",
            "  '/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/video/XIMEA_221020_23_trim/concat2/0039.jpg']\n",
            " [0\n",
            "  '/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/video/XIMEA_221020_23_trim/concat2/0042.jpg']\n",
            " [0\n",
            "  '/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/video/XIMEA_221020_23_trim/concat2/0049.jpg']\n",
            " [0\n",
            "  '/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/video/XIMEA_221020_23_trim/concat2/0043.jpg']\n",
            " [0\n",
            "  '/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/video/XIMEA_221020_23_trim/concat2/0036.jpg']]\n",
            "spatter label : : train : 748, test : 104\n",
            "1467\n"
          ]
        }
      ],
      "source": [
        "## ///////// loading the data as DataFrame ////////////\n",
        "#20220401\n",
        "#file=pd.read_csv(filepath_or_buffer=\"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/NN/dataset/train_dataset_2.csv\")\n",
        "#file_test = pd.read_csv(filepath_or_buffer= \"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/NN/dataset/test_dataset.csv\")\n",
        "\n",
        "#20220603\n",
        "file_train = pd.read_csv(filepath_or_buffer = \"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/dataset/train_vit.csv\")\n",
        "value_train = file_train.values\n",
        "file_test = pd.read_csv(filepath_or_buffer = \"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20221020/dataset/test_vit.csv\")\n",
        "value_test = file_test.values\n",
        "\n",
        "print(value_test[:5])\n",
        "## ////////// テスト用とトレーニング用でデータをシャッフルする //////////////\n",
        "#for 2D shuffle\n",
        "\n",
        "value_train = shuffle(value_train,random_state=36) #random_state = integer ; fix randomness with shuffling\n",
        "value_test = shuffle(value_test,random_state=42)\n",
        "#print(value_train.shape)\n",
        "#value_test = shuffle(value_test, random_state =42)\n",
        "#print(value_test.shape)\n",
        "\n",
        "## //////////// Show example /////////\n",
        "#img=cv2.imread(value_train[100,1])\n",
        "#gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#print(gray.shape)\n",
        "#plt.imshow(gray)\n",
        "\n",
        "## ////// making the dataset for train and test //////////\n",
        "\n",
        "data_train = value_train[:,1:]\n",
        "label_train =  value_train[:,0]\n",
        "\n",
        "\n",
        "data_test = value_test[:,1:]\n",
        "label_test = value_test[:,0]\n",
        "\n",
        "count_test = 0\n",
        "count_train = 0\n",
        "for i in range(len(label_test)):\n",
        "  if label_test[i] == 1:\n",
        "    count_test += 1\n",
        "\n",
        "for i in range(len(label_train)):\n",
        "  if label_train[i] == 1:\n",
        "    count_train += 1\n",
        "\n",
        "print('spatter label : : train : {}, test : {}'.format(count_train,count_test))\n",
        "#print(data_test)\n",
        "print(len(label_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4JqIuBMwbTp"
      },
      "source": [
        "##Convert URL to Images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQBI6ZfrJoiL",
        "outputId": "4af67ea6-dc78-4245-ae9b-72916644d0ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1467, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ezbnLI9uqwME"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 64*3\n",
        "def url2image(data,label,img_size=IMG_SIZE):\n",
        "  image_data = np.zeros((len(data),img_size,img_size))\n",
        "  label_revised = []\n",
        "  for i,[name] in enumerate(data):\n",
        "    print((len(data),i))\n",
        "    #print(name)\n",
        "    img=cv2.imread(name)\n",
        "    if img:\n",
        "      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "      image_data[i] = gray\n",
        "      label_revised.append(label[i])\n",
        "  print(\"length of dataset is \",len(label_revised))\n",
        "  imagedata = image_data[:len(label_revised)]\n",
        "\n",
        "  return image_data,label_revised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "P6CwGWDeq5W2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "0daf47f7-62d9-443a-94fb-515ab1203d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1467, 0)\n",
            "(1467, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-712902d18da8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-f215b57b4f73>\u001b[0m in \u001b[0;36murl2image\u001b[0;34m(data, label, img_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ],
      "source": [
        "image_train,label_train = url2image(data_train,label_train)\n",
        "image_test,label_test = url2image(data_test,label_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2yZ1RtCwfSf"
      },
      "source": [
        "\n",
        "##Process "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KFFrDphq7rH",
        "outputId": "2ba6bb45-d1d4-46cc-a8d5-831502a0444d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of label test :  460\n",
            "(3755, 10, 64, 64, 1)\n",
            "3755\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "## split data into training data and test data \n",
        "image_train = image_train.reshape(len(image_train),10,64,64,1)\n",
        "image_test = image_test.reshape(len(image_test),10,64,64,1)\n",
        "\n",
        "#when calculating (dividing process) the type have to be float\n",
        "image_train = image_train.astype(\"float32\")\n",
        "image_test = image_test.astype(\"float32\")\n",
        "\n",
        "#when training, it is convenient if the values are normalized\n",
        "image_train= image_train/255\n",
        "image_test = image_test/255\n",
        "\n",
        "#for validation data\n",
        "#num_sample = int(len(image_train)*0.8)\n",
        "#image_train = image_train#[:num_sample]\n",
        "#image_val = image_train#[num_sample:]\n",
        "#abel_train = label_train[:num_sample]\n",
        "#label_val = label_train[num_sample:]\n",
        "\n",
        "#converting data to float32, especially float32\n",
        "image_train =np.asarray(image_train).astype(np.float32)\n",
        "label_train = np.asarray(label_train).astype(np.int32)\n",
        "image_test =np.asarray(image_test).astype(np.float32)\n",
        "label_test = np.asarray(label_test).astype(np.int32)\n",
        "\n",
        "\n",
        "print(\"length of label test : \",len(label_test))\n",
        "\n",
        "#label_train_tensorflow=keras.utils.to_categorical(label_train)  #keras.utils.to_categorical : can see how the result of the judge is like [0.4,0.6] : no-spatter: 40%, spatter : 60%\n",
        "#label_test_tensorflow=keras.utils.to_categorical(label_test)\n",
        "#print(image_train)\n",
        "print(image_train.shape)\n",
        "print(len(label_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imkeIj7d7Oas",
        "outputId": "2371951e-2adc-463b-9e0f-f86edfba5343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spatter label:1966\n",
            "spatter label:190\n",
            "lab_train.shape ;  (3755, 2)\n",
            "lab_test.shape :  (460, 2)\n"
          ]
        }
      ],
      "source": [
        "# convert label into array(2) [no-spatter, spatter]\n",
        "def label_vec(label,dim):\n",
        "  count=0\n",
        "  lab = np.zeros((len(label),dim))  \n",
        "  for i in range(len(label)):\n",
        "    if label[i] == 1: #spatter\n",
        "      lab[i,1] = 1\n",
        "      count+=1\n",
        "    else: #no spatter\n",
        "      lab[i,0] = 1\n",
        "  print('spatter label:{}'.format(count))\n",
        "  return lab\n",
        "\n",
        "lab_train = label_vec(label_train, 2)\n",
        "lab_test = label_vec(label_test,2)\n",
        "\n",
        "print(\"lab_train.shape ; \" , lab_train.shape)\n",
        "print(\"lab_test.shape : \" , lab_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Setting"
      ],
      "metadata": {
        "id": "H7uAGSGKdIQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input Layer"
      ],
      "metadata": {
        "id": "m6HBcp1edJ_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Lrko25TNdYNl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IN_CHANNEL = 1\n",
        "EMB_DIM = 384\n",
        "NUM_PATCH_ROW = 3\n",
        "IMG_SIZE = 64*3\n",
        "\n",
        "class VitInputLayer(nn.Module): \n",
        "    def __init__(self, in_channels:int=IN_CHANNEL, emb_dim:int=EMB_DIM, num_patch_row:int=NUM_PATCH_ROW, image_size:int=IMG_SIZE):\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            in_channels: 入力画像のチャンネル数\n",
        "            emb_dim: 埋め込み後のベクトルの長さ\n",
        "            num_patch_row: 高さ方向のパッチの数。例は2x2であるため、2をデフォルト値とした \n",
        "            image_size: 入力画像の1辺の大きさ。入力画像の高さと幅は同じであると仮定\n",
        "        \"\"\"\n",
        "        super(VitInputLayer, self).__init__() \n",
        "        self.in_channels=in_channels \n",
        "        self.emb_dim = emb_dim \n",
        "        self.num_patch_row = num_patch_row \n",
        "        self.image_size = image_size\n",
        "        \n",
        "        # パッチの数\n",
        "        ## 例: 入力画像を2x2のパッチに分ける場合、num_patchは4 \n",
        "        self.num_patch = self.num_patch_row**2\n",
        "\n",
        "        # パッチの大きさ\n",
        "        ## 例: 入力画像の1辺の大きさが32の場合、patch_sizeは16 \n",
        "        self.patch_size = int(self.image_size // self.num_patch_row)\n",
        "\n",
        "        # 入力画像のパッチへの分割 & パッチの埋め込みを一気に行う層 \n",
        "        self.patch_emb_layer = nn.Conv2d(\n",
        "            in_channels=self.in_channels, \n",
        "            out_channels=self.emb_dim, \n",
        "            kernel_size=self.patch_size, \n",
        "            stride=self.patch_size\n",
        "        )\n",
        "\n",
        "        # クラストークン \n",
        "        self.cls_token = nn.Parameter(\n",
        "            torch.randn(1, 1, emb_dim) \n",
        "        )\n",
        "\n",
        "        # 位置埋め込み\n",
        "        ## クラストークンが先頭に結合されているため、\n",
        "        ## 長さemb_dimの位置埋め込みベクトルを(パッチ数+1)個用意 \n",
        "        self.pos_emb = nn.Parameter(\n",
        "            torch.randn(1, self.num_patch+1, emb_dim) \n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            x: 入力画像。形状は、(B, C, H, W)。[式(1)]\n",
        "                B: バッチサイズ、C:チャンネル数、H:高さ、W:幅\n",
        "\n",
        "        返り値:\n",
        "            z_0: ViTへの入力。形状は、(B, N, D)。\n",
        "                B:バッチサイズ、N:トークン数、D:埋め込みベクトルの長さ\n",
        "        \"\"\"\n",
        "        # パッチの埋め込み & flatten [式(3)]\n",
        "        ## パッチの埋め込み (B, C, H, W) -> (B, D, H/P, W/P) \n",
        "        ## ここで、Pはパッチ1辺の大きさ\n",
        "        z_0 = self.patch_emb_layer(x)\n",
        "\n",
        "        ## パッチのflatten (B, D, H/P, W/P) -> (B, D, Np) \n",
        "        ## ここで、Npはパッチの数(=H*W/Pˆ2)\n",
        "        z_0 = z_0.flatten(2)\n",
        "\n",
        "        ## 軸の入れ替え (B, D, Np) -> (B, Np, D) \n",
        "        z_0 = z_0.transpose(1, 2)\n",
        "\n",
        "        # パッチの埋め込みの先頭にクラストークンを結合 [式(4)] \n",
        "        ## (B, Np, D) -> (B, N, D)\n",
        "        ## N = (Np + 1)であることに留意\n",
        "        ## また、cls_tokenの形状は(1,1,D)であるため、\n",
        "        ## repeatメソッドによって(B,1,D)に変換してからパッチの埋め込みとの結合を行う \n",
        "        z_0 = torch.cat(\n",
        "            [self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1)\n",
        "\n",
        "        # 位置埋め込みの加算 [式(5)] \n",
        "        ## (B, N, D) -> (B, N, D) \n",
        "        z_0 = z_0 + self.pos_emb\n",
        "        return z_0\n",
        "\n",
        "batch_size, channel, height, width= 32, 1, 192, 192\n",
        "x = torch.randn(batch_size, channel, height, width) \n",
        "input_layer = VitInputLayer(num_patch_row=2) \n",
        "z_0=input_layer(x)\n",
        "\n",
        "# (2, 5, 384)(=(B, N, D))になっていることを確認。 \n",
        "print(z_0.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8v35xz7dJet",
        "outputId": "16521331-724f-4327-aa71-60ec1eb66c28"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Head Attention"
      ],
      "metadata": {
        "id": "_X7Upsq-eFnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_HEAD = 3\n",
        "DROPOUT = 0.0\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module): \n",
        "    def __init__(self, emb_dim:int=EMB_DIM, head:int=NUM_HEAD, dropout:float=DROPOUT):\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            emb_dim: 埋め込み後のベクトルの長さ \n",
        "            head: ヘッドの数\n",
        "            dropout: ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(MultiHeadSelfAttention, self).__init__() \n",
        "        self.head = head\n",
        "        self.emb_dim = emb_dim\n",
        "        self.head_dim = emb_dim // head\n",
        "        self.sqrt_dh = self.head_dim**0.5 # D_hの二乗根。qk^Tを割るための係数\n",
        "\n",
        "        # 入力をq,k,vに埋め込むための線形層。 [式(6)] \n",
        "        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False) \n",
        "        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False) \n",
        "        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "        # 式(7)にはないが、実装ではドロップアウト層も用いる \n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "\n",
        "        # MHSAの結果を出力に埋め込むための線形層。[式(10)]\n",
        "        ## 式(10)にはないが、実装ではドロップアウト層も用いる \n",
        "        self.w_o = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Dropout(dropout) \n",
        "        )\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            z: MHSAへの入力。形状は、(B, N, D)。\n",
        "                B: バッチサイズ、N:トークンの数、D:ベクトルの長さ\n",
        "\n",
        "        返り値:\n",
        "            out: MHSAの出力。形状は、(B, N, D)。[式(10)]\n",
        "                B:バッチサイズ、N:トークンの数、D:埋め込みベクトルの長さ\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, num_patch, _ = z.size()\n",
        "\n",
        "        # 埋め込み [式(6)]\n",
        "        ## (B, N, D) -> (B, N, D)\n",
        "        q = self.w_q(z)\n",
        "        k = self.w_k(z)\n",
        "        v = self.w_v(z)\n",
        "\n",
        "        # q,k,vをヘッドに分ける [式(10)]\n",
        "        ## まずベクトルをヘッドの個数(h)に分ける\n",
        "        ## (B, N, D) -> (B, N, h, D//h)\n",
        "        q = q.view(batch_size, num_patch, self.head, self.head_dim)\n",
        "        k = k.view(batch_size, num_patch, self.head, self.head_dim)\n",
        "        v = v.view(batch_size, num_patch, self.head, self.head_dim)\n",
        "\n",
        "        ## Self-Attentionができるように、\n",
        "        ## (バッチサイズ、ヘッド、トークン数、パッチのベクトル)の形に変更する \n",
        "        ## (B, N, h, D//h) -> (B, h, N, D//h)\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # 内積 [式(7)]\n",
        "        ## (B, h, N, D//h) -> (B, h, D//h, N)\n",
        "        k_T = k.transpose(2, 3)\n",
        "        ## (B, h, N, D//h) x (B, h, D//h, N) -> (B, h, N, N) \n",
        "        dots = (q @ k_T) / self.sqrt_dh\n",
        "        ## 列方向にソフトマックス関数\n",
        "        attn = F.softmax(dots, dim=-1)\n",
        "        ## ドロップアウト\n",
        "        attn = self.attn_drop(attn)\n",
        "        # 加重和 [式(8)]\n",
        "        ## (B, h, N, N) x (B, h, N, D//h) -> (B, h, N, D//h) \n",
        "        out = attn @ v\n",
        "        ## (B, h, N, D//h) -> (B, N, h, D//h)\n",
        "        out = out.transpose(1, 2)\n",
        "        ## (B, N, h, D//h) -> (B, N, D)\n",
        "        out = out.reshape(batch_size, num_patch, self.emb_dim)\n",
        "\n",
        "        # 出力層 [式(10)]\n",
        "        ## (B, N, D) -> (B, N, D) \n",
        "        out = self.w_o(out) \n",
        "        return out\n",
        "\n",
        "mhsa = MultiHeadSelfAttention()\n",
        "out = mhsa(z_0) #z_0は2-2節のz_0=input_layer(x)で、形状は(B, N, D)\n",
        "\n",
        "# (2, 5, 384)(=(B, N, D))になっていることを確認 \n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJfaVNiSdS8q",
        "outputId": "fc340e8b-6576-4086-a206-648562e1ffce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder"
      ],
      "metadata": {
        "id": "8AShL-EHeYld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VitEncoderBlock(nn.Module): \n",
        "    def __init__(self, emb_dim:int=EMB_DIM, head:int=NUM_HEAD, hidden_dim:int=EMB_DIM*4, dropout: float=DROPOUT):\n",
        "        \"\"\"\n",
        "        引数:\n",
        "            emb_dim: 埋め込み後のベクトルの長さ\n",
        "            head: ヘッドの数\n",
        "            hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ \n",
        "                        原論文に従ってemb_dimの4倍をデフォルト値としている\n",
        "            dropout: ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(VitEncoderBlock, self).__init__()\n",
        "        # 1つ目のLayer Normalization [2-5-2項]\n",
        "        self.ln1 = nn.LayerNorm(emb_dim)\n",
        "        # MHSA [2-4-7項]\n",
        "        self.msa = MultiHeadSelfAttention(\n",
        "        emb_dim=emb_dim, head=head,\n",
        "        dropout = dropout,\n",
        "        )\n",
        "        # 2つ目のLayer Normalization [2-5-2項] \n",
        "        self.ln2 = nn.LayerNorm(emb_dim)\n",
        "        # MLP [2-5-3項]\n",
        "        self.mlp = nn.Sequential( \n",
        "            nn.Linear(emb_dim, hidden_dim), \n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout), \n",
        "            nn.Linear(hidden_dim, emb_dim), \n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            z: Encoder Blockへの入力。形状は、(B, N, D)\n",
        "                B: バッチサイズ、N:トークンの数、D:ベクトルの長さ\n",
        "\n",
        "        返り値:\n",
        "            out: Encoder Blockへの出力。形状は、(B, N, D)。[式(10)]\n",
        "                B:バッチサイズ、N:トークンの数、D:埋め込みベクトルの長さ \n",
        "        \"\"\"\n",
        "        # Encoder Blockの前半部分 [式(12)] \n",
        "        out = self.msa(self.ln1(z)) + z\n",
        "        # Encoder Blockの後半部分 [式(13)] \n",
        "        out = self.mlp(self.ln2(out)) + out \n",
        "        return out\n",
        "\n",
        "vit_enc = VitEncoderBlock()\n",
        "z_1 = vit_enc(z_0) #z_0は2-2節のz_0=input_layer(x)で、形状は(B, N, D)\n",
        "\n",
        "# (2, 5, 384)(=(B, N, D))になっていることを確認 \n",
        "print(z_1.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESSAvB-ieWOc",
        "outputId": "fcd372d6-2e21-4bd2-9ce0-04a161900e32"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ViT"
      ],
      "metadata": {
        "id": "1wK-izggex0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 2\n",
        "NUM_BLOCK = 7\n",
        "HID_DIM = EMB_DIM * 4\n",
        "IN_CHANNEL = 1\n",
        "EMB_DIM = 384\n",
        "NUM_PATCH_ROW = 3\n",
        "IMG_SIZE = 64*3\n",
        "NUM_HEAD = 8\n",
        "DROPOUT = 0.0\n",
        "\n",
        "class Vit(nn.Module): \n",
        "    def __init__(self, in_channels:int=IN_CHANNEL, num_classes:int=NUM_CLASSES, emb_dim:int=EMB_DIM, num_patch_row:int=NUM_PATCH_ROW, image_size:int=IMG_SIZE, num_blocks:int=NUM_BLOCK, head:int=NUM_HEAD, hidden_dim:int=HID_DIM, dropout:float=DROPOUT):\n",
        "        \"\"\" \n",
        "        引数:\n",
        "            in_channels: 入力画像のチャンネル数\n",
        "            num_classes: 画像分類のクラス数\n",
        "            emb_dim: 埋め込み後のベクトルの長さ\n",
        "            num_patch_row: 1辺のパッチの数\n",
        "            image_size: 入力画像の1辺の大きさ。入力画像の高さと幅は同じであると仮定 \n",
        "            num_blocks: Encoder Blockの数\n",
        "            head: ヘッドの数\n",
        "            hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ \n",
        "            dropout: ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(Vit, self).__init__()\n",
        "        # Input Layer [2-3節] \n",
        "        self.input_layer = VitInputLayer(\n",
        "            in_channels, \n",
        "            emb_dim, \n",
        "            num_patch_row, \n",
        "            image_size)\n",
        "\n",
        "        # Encoder。Encoder Blockの多段。[2-5節] \n",
        "        self.encoder = nn.Sequential(*[\n",
        "            VitEncoderBlock(\n",
        "                emb_dim=emb_dim,\n",
        "                head=head,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout = dropout\n",
        "            )\n",
        "            for _ in range(num_blocks)])\n",
        "\n",
        "        # MLP Head [2-6-1項] \n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_dim),\n",
        "            nn.Linear(emb_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        引数:\n",
        "            x: ViTへの入力画像。形状は、(B, C, H, W)\n",
        "                B: バッチサイズ、C:チャンネル数、H:高さ、W:幅\n",
        "\n",
        "        返り値:\n",
        "            out: ViTの出力。形状は、(B, M)。[式(10)]\n",
        "                B:バッチサイズ、M:クラス数 \n",
        "        \"\"\"\n",
        "        # Input Layer [式(14)]\n",
        "        ## (B, C, H, W) -> (B, N, D)\n",
        "        ## N: トークン数(=パッチの数+1), D: ベクトルの長さ \n",
        "        out = self.input_layer(x)\n",
        "        \n",
        "        # Encoder [式(15)、式(16)]\n",
        "        ## (B, N, D) -> (B, N, D)\n",
        "        out = self.encoder(out)\n",
        "\n",
        "        # クラストークンのみ抜き出す\n",
        "        ## (B, N, D) -> (B, D)\n",
        "        cls_token = out[:,0]\n",
        "\n",
        "        # MLP Head [式(17)]\n",
        "        ## (B, D) -> (B, M)\n",
        "        pred = self.mlp_head(cls_token)\n",
        "        return pred\n",
        "\n",
        "num_classes = 10\n",
        "batch_size, channel, height, width= 2, 3, 32, 32\n",
        "x = torch.randn(batch_size, channel, height, width)\n",
        "vit = Vit(in_channels=channel, num_classes=num_classes) \n",
        "pred = vit(x)\n",
        "\n",
        "# (2, 10)(=(B, M))になっていることを確認 \n",
        "print(pred.shape)"
      ],
      "metadata": {
        "id": "uVcHj6WLeePa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trainer"
      ],
      "metadata": {
        "id": "AarsdroIfhvG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvOrSQGafiwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device : \",device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2wnDwsR-WFV",
        "outputId": "51ea4f7e-068b-4351-e01e-d77bf5bc2ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device :  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PADQiKNa2snb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "rng = np.random.RandomState(1234)\n",
        "random_state = 42\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "conv_net = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "\"\"\"\n",
        "conv_net = nn.Sequential(\n",
        "    nn.Conv2d(3, 8, 3,padding=1),              \n",
        "    nn.BatchNorm2d(8),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),                  # 16x16x32\n",
        "    nn.Conv2d(8, 32, 3,padding=1),   \n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),                  # 8*8*32\n",
        "    nn.Conv2d(32, 128, 3,padding=1),            \n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),                  # 4x4x128\n",
        "    nn.Conv2d(128, 512, 3,padding=1),            \n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),                  # 2x2x512\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(2048, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 10),\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def init_weights(m):  # Heの初期化\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        torch.nn.init.kaiming_normal_(m.weight)\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "conv_net.apply(init_weights)\n",
        "\n",
        "\n",
        "n_epochs = 15\n",
        "lr = 0.01\n",
        "device = 'cuda'\n",
        "\n",
        "conv_net.to(device)\n",
        "optimizer = optimizer = torch.optim.RMSprop(conv_net.parameters(), lr=lr)#torch.optim.Adam(conv_net.parameters(),lr=lr) # WRITE ME\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "# WRITE ME\n",
        "loss_function = nn.CrossEntropyLoss()# WRITE ME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlOZuLu-328i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49273fce-cf8e-4f39-c0c8-d56a058544a1"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "\n",
        "    conv_net.train()\n",
        "    n_train = 0\n",
        "    acc_train = 0\n",
        "    for x, t in dataloader_train:\n",
        "      n_train += t.size()[0]\n",
        "\n",
        "      conv_net.zero_grad()\n",
        "\n",
        "      x = x.to(device)\n",
        "      t = t.to(device)\n",
        "\n",
        "      y = conv_net.forward(x)\n",
        "\n",
        "      loss  = loss_function(y,t)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      pred = y.argmax(1)\n",
        "\n",
        "\n",
        "      acc_train += (pred == t).float().sum().item()\n",
        "      losses_train.append(loss.tolist())\n",
        "\n",
        "    conv_net.eval()\n",
        "    n_val = 0\n",
        "    acc_val = 0\n",
        "    for x, t in dataloader_valid:\n",
        "        n_val += t.size()[0]\n",
        "\n",
        "        t = t.to(device)\n",
        "        x = x.to(device)\n",
        "\n",
        "        y = conv_net.forward(x)\n",
        "\n",
        "        loss = loss_function(y,t)\n",
        "\n",
        "        pred = y.argmax(1)\n",
        "\n",
        "\n",
        "        acc_val += (pred == t).float().sum().item()\n",
        "        losses_valid.append(loss.tolist())\n",
        "    scheduler.step()\n",
        "    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}], learning_rate : {}'.format(\n",
        "        epoch,\n",
        "        np.mean(losses_train),\n",
        "        acc_train/n_train,\n",
        "        np.mean(losses_valid),\n",
        "        acc_val/n_val,\n",
        "        optimizer.param_groups[0][\"lr\"]\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0, Train [Loss: 1.910, Accuracy: 0.328], Valid [Loss: 1.511, Accuracy: 0.447], learning_rate : 0.01\n",
            "EPOCH: 1, Train [Loss: 1.260, Accuracy: 0.547], Valid [Loss: 1.633, Accuracy: 0.471], learning_rate : 0.01\n",
            "EPOCH: 2, Train [Loss: 1.075, Accuracy: 0.618], Valid [Loss: 1.248, Accuracy: 0.595], learning_rate : 0.01\n",
            "EPOCH: 3, Train [Loss: 0.955, Accuracy: 0.664], Valid [Loss: 1.159, Accuracy: 0.595], learning_rate : 0.01\n",
            "EPOCH: 4, Train [Loss: 0.868, Accuracy: 0.696], Valid [Loss: 1.042, Accuracy: 0.634], learning_rate : 0.01\n",
            "EPOCH: 5, Train [Loss: 0.805, Accuracy: 0.721], Valid [Loss: 1.046, Accuracy: 0.648], learning_rate : 0.01\n",
            "EPOCH: 6, Train [Loss: 0.742, Accuracy: 0.740], Valid [Loss: 1.227, Accuracy: 0.635], learning_rate : 0.01\n",
            "EPOCH: 7, Train [Loss: 0.695, Accuracy: 0.757], Valid [Loss: 0.933, Accuracy: 0.689], learning_rate : 0.01\n",
            "EPOCH: 8, Train [Loss: 0.648, Accuracy: 0.773], Valid [Loss: 1.373, Accuracy: 0.577], learning_rate : 0.01\n",
            "EPOCH: 9, Train [Loss: 0.606, Accuracy: 0.788], Valid [Loss: 0.861, Accuracy: 0.713], learning_rate : 0.001\n",
            "EPOCH: 10, Train [Loss: 0.432, Accuracy: 0.852], Valid [Loss: 0.824, Accuracy: 0.738], learning_rate : 0.001\n",
            "EPOCH: 11, Train [Loss: 0.375, Accuracy: 0.870], Valid [Loss: 0.850, Accuracy: 0.735], learning_rate : 0.001\n",
            "EPOCH: 12, Train [Loss: 0.344, Accuracy: 0.880], Valid [Loss: 0.896, Accuracy: 0.737], learning_rate : 0.001\n",
            "EPOCH: 13, Train [Loss: 0.323, Accuracy: 0.888], Valid [Loss: 0.931, Accuracy: 0.729], learning_rate : 0.001\n",
            "EPOCH: 14, Train [Loss: 0.305, Accuracy: 0.894], Valid [Loss: 0.966, Accuracy: 0.724], learning_rate : 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq3scS5j4Rt2"
      },
      "source": [
        "conv_net.eval()\n",
        "\n",
        "t_pred = []\n",
        "for x in dataloader_test:\n",
        "\n",
        "    x = x.to(device)\n",
        "\n",
        "    # 順伝播\n",
        "    y = conv_net.forward(x)\n",
        "\n",
        "    # モデルの出力を予測値のスカラーに変換\n",
        "    pred = y.argmax(1).tolist()\n",
        "\n",
        "    t_pred.extend(pred)\n",
        "\n",
        "submission = pd.Series(t_pred, name='label')\n",
        "submission.to_csv('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture05/submission_pred.csv', header=True, index_label='id')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}